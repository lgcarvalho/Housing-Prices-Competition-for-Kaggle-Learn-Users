{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Housing Prices Competition for Kaggle Learn Users**\n",
    "\n",
    "___\n",
    "\n",
    "## Competition Description.\n",
    "\n",
    "Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Goal\n",
    "It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n",
    "\n",
    "### Metric\n",
    "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
    "\n",
    "### Submission File Format\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "Id,SalePrice\n",
    "<br>\n",
    "1461,169000.1\n",
    "<br>\n",
    "1462,187724.1233\n",
    "<br>\n",
    "1463,175221\n",
    "<br>\n",
    "etc.\n",
    "\n",
    "There is a sample file called 'sample_submission' in 'Base de Dados' folder.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importando as bibliotecas necessárias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lendo o arquivo de entrada:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_arquivo = '../Base de Dados/train.csv'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "house_price_data = pd.read_csv(caminho_arquivo, index_col=0)\n",
    "house_price_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Análise exploratória dos dados:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observações preliminares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O \"shape\" do dataset está mostrando que ele possui 1460 linhas e 80 colunas, sendo que a coluna [SalePrice] é o target que o modelo deve prever.\n",
    "Ou seja, tem 79 colunas que podem ser usadas para a seleção dos recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colunas Númericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das colunas númericas\n",
    "colunas_numericas = house_price_data.select_dtypes(exclude=['object'])\n",
    "colunas_numericas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de colunas númericas\n",
    "len(colunas_numericas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima mostra que, aparentemente, tem 37 colunas do tipo númerico, incluindo a target [SalePrice].\n",
    "\n",
    "- Algumas anomalias nas colunas do dataset podem fazer com que o tipo de dado da coluna se torne 'object', isso pode ocasionar em uma separação errada das colunas númericas e categoricas.\n",
    "- Também é possível que tenham colunas que possuem dados em uma forma mais discreta e números com valores limitados. Essas colunas devem ser interpretadas, também, como categoricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 37 colunas numéricas possuem as seguintes caracteristicas gerais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_numericas.describe().round(decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colunas Categoricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_categoricas = house_price_data.select_dtypes(include=['object'])\n",
    "colunas_categoricas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(colunas_categoricas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 43 colunas categoricas com as seguintes caracteristicas gerais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_categoricas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando as colunas númericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distorção da coluna target**\n",
    "\n",
    "Aparentemente é uma boa prática minimizar a distorção do dataset. A razão é que, geralmente, os dados distorcidos afetam negativamente a precisão da previsão dos modelos de regressão.\n",
    "Nota: Embora seja importante para a regressão linear, corrigir a distorção não é necessária para a Árvore de Decisão ou Florestas Aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = house_price_data.SalePrice\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.distplot(target)\n",
    "\n",
    "plt.title('Distribuição da SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(target))\n",
    "\n",
    "plt.title('Distribuição do Log-transformado SalePrice')\n",
    "plt.xlabel('log(SalePrice)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SalePrice possui uma distorção de ' + str(target.skew().round(decimals=2)) + ' enquando o log-transformado SalePrice melhora a distorção para ' + str(np.log(target).skew().round(decimals=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribuição das colunas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_numericas = house_price_data.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\n",
    "\n",
    "fig = plt.figure(figsize=(12,18))\n",
    "for i in range(len(colunas_numericas.columns)):\n",
    "    fig.add_subplot(9,4,i+1)\n",
    "    sns.distplot(colunas_numericas.iloc[:,i].dropna())\n",
    "    plt.xlabel(colunas_numericas.columns[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível fazer um log transformado nas seguinte distorções:\n",
    "\n",
    "LotFrontage, LotArea, stFlrSF, GrLivArea, OpenPorchSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscando Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise univariada - box plots para atributos numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 18))\n",
    "\n",
    "for i in range(len(colunas_numericas.columns)):\n",
    "    fig.add_subplot(9, 4, i + 1)\n",
    "    \n",
    "    sns.boxplot(y=colunas_numericas.iloc[:, i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise bivariada - gráficos de dispersão para atributos alvo versus atributos numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,20))\n",
    "\n",
    "for i in range(len(colunas_numericas.columns)):\n",
    "    f.add_subplot(9, 4, i+1)\n",
    "    sns.scatterplot(x=colunas_numericas.iloc[:,i], y=target)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma primeira vista do scatter plots contra o SalePrice, parecer ter outliers nas seguintes colunas:\n",
    "\n",
    "- LotFrontage\n",
    "- LotArea\n",
    "- BsmtFinSF1\n",
    "- TotalBsmtSF\n",
    "- 1stFlrSF\n",
    "- GrLivArea\n",
    "- LowQualFinSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlações entre as colunas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacao = house_price_data.corr(numeric_only=True)\n",
    "\n",
    "f, asx = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "plt.title('Correlação das colunas numericas', size=16)\n",
    "\n",
    "sns.heatmap(correlacao)\n",
    "# sns.heatmap(correlacao, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando como referência o target SalePrice, as top 15 correlações são:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacao['SalePrice'].sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valores faltantes ou nulos nas colunas númericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar a coluna com a maior quantidade de nulos\n",
    "colunas_numericas.isna().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando as colunas categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_categoricas = house_price_data.select_dtypes(include='object').columns\n",
    "\n",
    "print(colunas_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = house_price_data['KitchenQual']\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(y=house_price_data.SalePrice, x=var)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(y=house_price_data.SalePrice, x=house_price_data.Neighborhood)\n",
    "\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12.5, 4))\n",
    "\n",
    "sns.countplot(x='Neighborhood', data=house_price_data)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valores faltantes ou nulos nas colunas categoricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data[colunas_categoricas].isna().sum().sort_values(ascending=False).head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limpeza dos dados e pré-processamento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com valores faltantes ou nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia do dataset\n",
    "house_price_data_copy = house_price_data.copy()\n",
    "\n",
    "house_price_data_copy.MasVnrArea = house_price_data_copy.MasVnrArea.fillna(0)\n",
    "house_price_data_copy.LotFrontage = house_price_data_copy.LotFrontage.fillna(0)\n",
    "house_price_data_copy.GarageYrBlt = house_price_data_copy.GarageYrBlt.fillna(0)\n",
    "\n",
    "colunas_categoricas_preenche_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n",
    "                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n",
    "                     'MasVnrType', 'Electrical']\n",
    "\n",
    "for categoria in colunas_categoricas_preenche_none:\n",
    "    house_price_data_copy[categoria] = house_price_data_copy[categoria].fillna('None')\n",
    "    \n",
    "house_price_data_copy.isna().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com os outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo os outliers de acordo com as observações do scatter plot contra a coluna SalePrice\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['LotFrontage'][house_price_data_copy['LotFrontage']>200].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['LotArea'][house_price_data_copy['LotArea']>100000].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['BsmtFinSF1'][house_price_data_copy['BsmtFinSF1']>4000].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['TotalBsmtSF'][house_price_data_copy['TotalBsmtSF']>6000].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['1stFlrSF'][house_price_data_copy['1stFlrSF']>4000].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['GrLivArea'][(house_price_data_copy['GrLivArea']>4000) & (target<300000)].index)\n",
    "house_price_data_copy = house_price_data_copy.drop(house_price_data_copy['LowQualFinSF'][house_price_data_copy['LowQualFinSF']>550].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando o dado para reduzir a distorção.\n",
    "\n",
    "No momento só vamos utilizar a váriavel target [SalePrice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data_copy['SalePrice'] = np.log(house_price_data_copy['SalePrice'])\n",
    "house_price_data_copy = house_price_data_copy.rename(columns={'SalePrice': 'SalePrice_Log'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validando como ficou o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection & Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerando correlações altas**\n",
    "\n",
    "Utilizando correlações altas para um algoritmo de machine learning pode causar a redução na performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacao_transformada = house_price_data_copy.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(correlacao_transformada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas com alta correlação entre si (a coluna da esquerda possui uma correlação maior com o SalePrice).\n",
    "\n",
    "- GarageCars and GarageArea (0.882)\n",
    "- YearBuild and GarageYrBlt (0.826)\n",
    "- GrLivArea and TotRmsAbvGrd (0.826)\n",
    "- TotalBsmtSF and '1stFlrSG' (0.780)\n",
    "\n",
    "Provavelmente devemos escolher, entre as que possuem mais de 0.8 na correlação, se fazemos um drop nas colunas que possuem a menor correlação contra a coluna target [SalePrice]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_desejadas = ['GarageCars', 'GarageArea', 'YearBuilt', 'GarageYrBlt', 'GrLivArea', 'TotRmsAbvGrd', 'TotalBsmtSF', '1stFlrSF']\n",
    "\n",
    "correlacao_transformada['SalePrice_Log'][colunas_desejadas].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realizando o feature scaling, e a transformação das colunas categoricas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as colunas identificadas no scatter plot e na correlação\n",
    "colunas_drop = ['SalePrice_Log', 'MiscVal', 'MSSubClass', 'MoSold', 'YrSold', 'GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd']\n",
    "\n",
    "X = house_price_data_copy.drop(colunas_drop, axis=1)\n",
    "y = house_price_data_copy.SalePrice_Log\n",
    "\n",
    "# OneHotEnconding para transformar as colunas categoricas\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Separando os dados em treinamento e teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalonamento e imputação nos dados de treinamento\n",
    "simple_imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "X_treino = standard_scaler.fit_transform(X_treino)\n",
    "X_treino = simple_imputer.fit_transform(X_treino)\n",
    "\n",
    "# Escalonamento e imputação nos dados de teste (sem ajuste)\n",
    "X_teste = standard_scaler.transform(X_teste)\n",
    "X_teste = simple_imputer.transform(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Avaliação preliminar de algoritmos de Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverte_y(y_transformado):\n",
    "    return np.exp(y_transformado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_teste = inverte_y(y_teste)\n",
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series to collate mean absolute errors for each algorithm\n",
    "mae_compare = pd.Series()\n",
    "mae_compare.index.name = 'Algorithm'\n",
    "\n",
    "# Specify Model ================================\n",
    "\n",
    "# Random Forest. =============================\n",
    "rf_model = RandomForestRegressor(random_state=5)\n",
    "rf_model.fit(X_treino, y_treino)\n",
    "rf_val_predictions = rf_model.predict(X_teste)\n",
    "rf_val_predictions = inverte_y(rf_val_predictions)\n",
    "rf_val_mae = mean_absolute_error(rf_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['RandomForest'] = rf_val_mae\n",
    "\n",
    "# XGBoost. Define the model. ======================================\n",
    "xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "xgb_model.fit(X_treino, y_treino, early_stopping_rounds=5, \n",
    "              eval_set=[(X_teste, y_teste)], verbose=False)\n",
    "xgb_val_predictions = xgb_model.predict(X_teste)\n",
    "xgb_val_predictions = inverte_y(xgb_val_predictions)\n",
    "xgb_val_mae = mean_absolute_error(xgb_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['XGBoost'] = xgb_val_mae\n",
    "\n",
    "# Árvore de decisão\n",
    "dt_model = DecisionTreeRegressor(max_depth=4)\n",
    "dt_model.fit(X_treino, y_treino)\n",
    "dt_val_predictions = dt_model.predict(X_teste)\n",
    "dt_val_predictions = inverte_y(dt_val_predictions)\n",
    "dt_val_mae = mean_absolute_error(dt_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['DecisionTree'] = dt_val_mae\n",
    "\n",
    "# Linear Regression =================================================\n",
    "# Para esse modelo o Linear Regression não funciona se for utilizado o StandardScaler\n",
    "#linear_model = LinearRegression()\n",
    "#linear_model.fit(X_treino, y_treino)\n",
    "#linear_val_predictions = linear_model.predict(X_teste)\n",
    "#linear_val_predictions = inverte_y(linear_val_predictions)\n",
    "#linear_val_mae = mean_absolute_error(linear_val_predictions, y_teste)\n",
    "\n",
    "#mae_compare['LinearRegression'] = linear_val_mae\n",
    "# print(\"Validation MAE for Linear Regression Model: {:,.0f}\".format(linear_val_mae))\n",
    "\n",
    "# Lasso ==============================================================\n",
    "lasso_model = Lasso(alpha=0.0005, random_state=5)\n",
    "lasso_model.fit(X_treino, y_treino)\n",
    "lasso_val_predictions = lasso_model.predict(X_teste)\n",
    "lasso_val_predictions = inverte_y(lasso_val_predictions)\n",
    "lasso_val_mae = mean_absolute_error(lasso_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['Lasso'] = lasso_val_mae\n",
    "\n",
    "# Ridge ===============================================================\n",
    "ridge_model = Ridge(alpha=0.002, random_state=5)\n",
    "ridge_model.fit(X_treino, y_treino)\n",
    "ridge_val_predictions = ridge_model.predict(X_teste)\n",
    "ridge_val_predictions = inverte_y(ridge_val_predictions)\n",
    "ridge_val_mae = mean_absolute_error(ridge_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['Ridge'] = ridge_val_mae\n",
    "\n",
    "# ElasticNet ===========================================================\n",
    "elastic_net_model = ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7)\n",
    "elastic_net_model.fit(X_treino, y_treino)\n",
    "elastic_net_val_predictions = elastic_net_model.predict(X_teste)\n",
    "elastic_net_val_predictions = inverte_y(elastic_net_val_predictions)\n",
    "elastic_net_val_mae = mean_absolute_error(elastic_net_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['ElasticNet'] = elastic_net_val_mae\n",
    "\n",
    "# Gradient Boosting Regression ==========================================\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, \n",
    "                                      max_depth=4, random_state=5)\n",
    "gbr_model.fit(X_treino, y_treino)\n",
    "gbr_val_predictions = gbr_model.predict(X_teste)\n",
    "gbr_val_predictions = inverte_y(gbr_val_predictions)\n",
    "gbr_val_mae = mean_absolute_error(gbr_val_predictions, y_teste)\n",
    "\n",
    "mae_compare['GradientBoosting'] = gbr_val_mae\n",
    "\n",
    "print('MAE values for different algorithms:')\n",
    "mae_compare.sort_values(ascending=True).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(mae_compare) + 1), mae_compare.values, color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)\n",
    "plt.xticks(range(1, len(mae_compare) + 1), mae_compare.index)\n",
    "plt.title('Avaliação Preliminar de Modelos de Machine Learning')\n",
    "plt.xlabel('Algoritmo')\n",
    "plt.ylabel('Erro Médio Quadrático')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validação cruzada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer()\n",
    "imputed_X = imputer.fit_transform(X)\n",
    "n_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lasso_model, imputed_X, y, scoring='neg_mean_squared_error', cv=n_folds)\n",
    "lasso_mae_scores = np.sqrt(-scores)\n",
    "\n",
    "print('For LASSO model:')\n",
    "print('Mean RMSE = ' + str(lasso_mae_scores.mean().round(decimals=3)))\n",
    "print('Error std deviation = ' + str(lasso_mae_scores.std().round(decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(gbr_model, imputed_X, y, scoring='neg_mean_squared_error', cv=n_folds)\n",
    "gbr_mae_scores = np.sqrt(-scores)\n",
    "\n",
    "print('For Gradient Boosting model:')\n",
    "print('Mean RMSE = ' + str(gbr_mae_scores.mean().round(decimals=3)))\n",
    "print('Error std deviation = ' + str(gbr_mae_scores.std().round(decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(xgb_model, imputed_X, y, scoring='neg_mean_squared_error', cv=n_folds)\n",
    "xgb_mae_scores = np.sqrt(-scores)\n",
    "\n",
    "print('For XGBoost model:')\n",
    "print('Mean RMSE = ' + str(xgb_mae_scores.mean().round(decimals=3)))\n",
    "print('Error std deviation = ' + str(xgb_mae_scores.std().round(decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf_model, imputed_X, y, scoring='neg_mean_squared_error', cv=n_folds)\n",
    "rf_mae_scores = np.sqrt(-scores)\n",
    "\n",
    "print('For Random Forest model:')\n",
    "print('Mean RMSE = ' + str(rf_mae_scores.mean().round(decimals=3)))\n",
    "print('Error std deviation = ' + str(rf_mae_scores.std().round(decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(dt_model, imputed_X, y, scoring='neg_mean_squared_error', cv=n_folds)\n",
    "dt_mae_scores = np.sqrt(-scores)\n",
    "\n",
    "print('For Decision Tree model:')\n",
    "print('Mean RMSE = ' + str(dt_mae_scores.mean().round(decimals=3)))\n",
    "print('Error std deviation = ' + str(dt_mae_scores.std().round(decimals=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Seleção do melhor algoritmo e ajustes finos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando um modelo e fazendo as predições**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando os melhores parâmetros do modelo Lasso\n",
    "param_grid = [{'alpha': [0.0007, 0.0005, 0.005]}]\n",
    "\n",
    "top_reg = Lasso()\n",
    "\n",
    "grid_search = GridSearchCV(top_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(imputed_X, y)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo que será utilizado para as predições.\n",
    "test_data_path = '../Base de Dados/test.csv'\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repetindo o pré-processamento definido anteriormente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste_predicao = test_data.copy()\n",
    "X_teste_predicao.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste_predicao.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas númericas\n",
    "X_teste_predicao.MasVnrArea = X_teste_predicao.MasVnrArea.fillna(0)\n",
    "\n",
    "# Retirando as colunas escolhidas\n",
    "for categoria in colunas_categoricas_preenche_none:\n",
    "    X_teste_predicao[categoria] = X_teste_predicao[categoria].fillna('None')\n",
    "    \n",
    "if 'SalePrice_Log' in colunas_drop:\n",
    "    colunas_drop.remove('SalePrice_Log')\n",
    "    \n",
    "X_teste_predicao = X_teste_predicao.drop(colunas_drop, axis=1)    \n",
    "\n",
    "# OneHotEncoder\n",
    "X_teste_predicao = pd.get_dummies(X_teste_predicao)\n",
    "\n",
    "# Tendo certeza que os dados de teste estão encodados da mesma forma que os dados de treino\n",
    "treino_final, teste_final = X.align(X_teste_predicao, join='left', axis=1)\n",
    "\n",
    "teste_final_imputed = simple_imputer.transform(teste_final)\n",
    "\n",
    "teste_final_imputed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando o modelo final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando o melhor modelo: Lasso\n",
    "modelo_final = Lasso(alpha=0.0005, random_state=42)\n",
    "\n",
    "treino_final_imputed = simple_imputer.fit_transform(treino_final)\n",
    "\n",
    "modelo_final.fit(treino_final_imputed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realizando a previsão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicao_teste = modelo_final.predict(teste_final_imputed)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': np.exp(predicao_teste)})\n",
    "\n",
    "output.to_csv('..\\Base de Dados\\submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_devs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
